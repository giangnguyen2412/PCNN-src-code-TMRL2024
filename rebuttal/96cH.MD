Thank you for your encouraging feedback and constructive suggestions. Please find our responses to your comments below.

> (Weaknesses 1) When compared to other existing classifiers, the propsed method is slower than k-NN and prototypical part-based classifiers but quicker than CHM-Corr and EMD-Corr re-rankers. Although it is in line with existing methods, its slower speed could be a drawback in situations where efficiency is crucial.

> (Requested Changes 1) Include a discussion on potential strategies to address the challenge of extended inference time.

We would like to respond to both of these concerns together because they are closely related.

After submitting the paper, we have been working on improving the efficiency of our method.
As the total runtime of our method is computed by: 

T = T_RN50 + T_kNN + T_S (Eq. 3 in paper) 

where T_RN50 is the time to do inference with pretrained classifier C (here RN50), T_kNN is the time to retrieve nearest neighbors, and T_S is the time to compute the similarity scores using S.
Below, we show that we can significantly reduce T_kNN and T_S while maintaining the accuracy of our method:

(1) Reducing T_kNN by shrinking training set during inference

We reduce the training set from 100% → 50% → 33% of the original size and observe the effect on the accuracy and runtime.
Here is the experimental data on CUB-200 and Dogs-120:

| Dataset   | % Data | Samples per Class | Top-1 Acc(%) | Runtime (s) |
|-----------|--------|-------------------|--------------|-------------|
| CUB-200   | 100%   | 30                | 88.43        | 64.55       |
| CUB-200   | 50%    | 15                | 88.26        | 59.70       |
| CUB-200   | 33%    | 10                | 88.19        | 58.08       |
| Dogs-120  | 100%   | 100               | 86.27        | 87.18       | 
| Dogs-120  | 50%    | 50                | 86.32        | 71.02       |
| Dogs-120  | 33%    | 33                | 86.42        | 65.52       |

**_The top-1 accuracy and runtime of CxS on CUB-200 and Dogs-120 for different sizes of training data during inference. Runtime was computed over 1000 samples, similar to the setup in Appendix E._**

When keeping the same accuracy, we can reduce the runtime by 10% on CUB-200 and 24.9% on Dogs-120 by reducing the training set to 33% of the original size.

(2) Reducing T_S by reducing the number of comparisons done by S 

We can reduce the number of queries to the image comparator S by ignoring less probable labels.
We run an experiment for CUB-200 where we instead of re-ranking the whole **top-10**, we only re-rank the classes that have a probability `>= 1%` assigned by the base classifier C.

We found that:
- The PoE model performance on CUB-200 drops very marginally by `only 0.08%` (from `88.59%` → `88.51%`).
- However, the number of queries to the image comparator S was reduced by approx. `4x` (from `10` to just about `2.5` queries/image).
This leads to a `2.5x` speedup in the overall runtime of the PoE system (from `64.55` seconds to `28.95` seconds per 1000 images), as shown in the following Table.

|               Model                |   Time (s)   | Top-1 Acc (%) |
|:----------------------------------:|:------------:|:-------------:|
|          RN50 xS (before)          | 64.55 ± 0.35 |     88.59     |
|          RN50 xS (after)           | 28.95 ± 0.11 |     88.51     |

**_The run-time of PoE on 1,000 queries on one Nvidia V100 GPU._**

We also include a discussion on potential strategies to further speed up runtime in response to the Reviewer `g7wm` [here](https://openreview.net/forum?id=OcFjqiJ98b&noteId=XuB3bY6d9q).
In this, we suggest to leverage GPU to speed up the nearest neighbor retrieval algorithm and use built-in options from `faiss` library to speed up the similarity computation.

> (Requested Changes 2) Can you determine an optimal value for K by considering the unique characteristics of each query or input, in order to enhance the balance between accuracy and runtime efficiency?

Thank you for such insightful comment!

This concern is relevant to a point raised by Reviewer `g7wm` [here](https://openreview.net/forum?id=OcFjqiJ98b&noteId=XuB3bY6d9q).
In this experiment, different inputs will take different K values based on how confident the classifier C is about the prediction.
However, we have done it relatively coarsely by only picking K based on an empirical threshold of confidence score at 1%.
By using these suboptimal K values, we can already reduce the runtime of the PoE system by 2.5x with a marginal drop (0.08%) in accuracy.

However, determining an optimal value for K based on the unique characteristics of each query or input requires `image understanding` that is still an ongoing research problem to our knowledge.
We believe that this is an interesting direction for future work and will include it in the discussion section of the next revision.

> (Requested Changes 3) Is it possible to implement incremental learning techniques to update the comparator S over time with new data?

Yes, it is possible to implement incremental learning techniques to update the comparator S over time with new data.
Comparator S is essentially a deep learning model that could work with popular incremental learning techniques approaches like replay, pseudo-rehearsal, or regularization.